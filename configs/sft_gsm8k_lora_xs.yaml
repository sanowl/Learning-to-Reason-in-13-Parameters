mode: sft
seed: 42
deterministic: false
deterministic_warn_only: true
model_name: Qwen/Qwen2.5-7B-Instruct
model_dtype: bfloat16
output_dir: outputs/sft_gsm8k_lora_xs

train_dataset:
  source: hf
  path: gsm8k
  name: main
  split: train
  prompt_field: question
  answer_field: answer
  prompt_template: "{question}\n\nReason step by step. End with only the final answer."

eval_dataset:
  source: hf
  path: gsm8k
  name: main
  split: test
  prompt_field: question
  answer_field: answer
  prompt_template: "{question}\n\nReason step by step. End with only the final answer."

adapter:
  adapter_type: lora_xs
  rank: 2
  tie_mode: full
  tie_factor: 1
  scale: 1.0
  svd_method: auto
  svd_niter: 2
  seed: 42
  vector_dtype: float32
  compute_dtype: float32
  target_modules: [q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj]

max_length: 4096

training:
  num_train_epochs: 3
  learning_rate: 5.0e-6
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 64
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 10
  save_steps: 200
  evaluation_strategy: no
  bf16: true
  fp16: false
  gradient_checkpointing: true
  report_to: []
